# Biomarker Implementation Summary

## What We Built

### Architecture Overview

```
BWH Data (signal_decays.json + metadata.csv)
    â†“
MCMC Inference (Gibbs or NUTS)
    â†“
Reconstructed Spectra with Posterior Samples
    â†“
Feature Extraction (with uncertainty)
    â†“
Gleason Score Classification
    â†“
Results + Feature Importance
```

---

## Key Files Created

### 1. **Core Module** (`src/spectra_estimation_dmri/biomarkers/simple_gleason_predictor.py`)

**Functions:**
- `extract_biomarker_features()` - Extract clinical biomarkers from spectrum
- `extract_features_from_dataset()` - Batch feature extraction
- `prepare_gleason_targets()` - Match spectra to Gleason scores
- `simple_biomarker_analysis()` - Complete pipeline

**Class:**
- `SimpleGleasonPredictor` - Wrapper around sklearn classifiers
  - `fit()`, `predict()`, `predict_proba()`
  - `predict_with_uncertainty()` - Monte Carlo predictions
  - `evaluate()`, `cross_validate()`
  - `plot_feature_importance()`

**Total: ~450 lines of clean, documented code**

### 2. **Standalone Script** (`scripts/run_bwh_biomarker_analysis.py`)

Complete end-to-end pipeline:
- Loads BWH data
- Runs MCMC inference
- Extracts biomarkers
- Trains classifier
- Saves results

**Usage:**
```bash
uv run python scripts/run_bwh_biomarker_analysis.py \
  --sampler gibbs \
  --n_iter 5000 \
  --use_mc
```

### 3. **Test Script** (`scripts/test_biomarker_workflow.py`)

Tests with synthetic data:
- âœ“ Feature extraction (with/without uncertainty)
- âœ“ Classifier training
- âœ“ Uncertainty features usage

**All tests passed!**

### 4. **Documentation**
- `BIOMARKER_ANALYSIS_GUIDE.md` - Complete usage guide
- `BIOMARKER_IMPLEMENTATION_SUMMARY.md` - This file

### 5. **Configuration** (`configs/biomarker/simple.yaml`)

```yaml
enabled: true
use_uncertainty: true
use_mc_predictions: false
gleason_threshold: 2
```

---

## How It Works: Two Approaches

### Approach 1: Uncertainty as Features (DEFAULT)

**Concept:** Add uncertainty metrics alongside point estimates

**Features Extracted:**
```python
Point Estimates:
- low_frac: 0.650           # Tumor marker
- high_frac: 0.200          # Normal tissue marker
- low_high_ratio: 3.25      # Key discriminator

Uncertainty Metrics:
- low_frac_std: 0.045       # How certain are we?
- low_high_ratio_std: 0.15
- low_frac_ci_width: 0.12   # 95% credible interval width
- spectrum_uncertainty: 0.03 # Global uncertainty
```

**Why this helps:**
- Classifier learns: "If `low_high_ratio_std` is high, be more cautious"
- Fast (single forward pass)
- Interpretable feature importance

**Test result:** Uncertainty features account for 11.5% of model importance âœ“

### Approach 2: Monte Carlo Predictions (OPTIONAL)

**Concept:** Use posterior samples directly for predictions

**How it works:**
1. For each patient, we have ~1000 MCMC samples of their spectrum
2. For each sample: extract features â†’ make prediction
3. Aggregate: `P(high-grade) = mean Â± std`

**Example output:**
```
Patient 1: P(high-grade) = 0.85 Â± 0.03  # High confidence â†’ tumor
Patient 2: P(high-grade) = 0.52 Â± 0.18  # Uncertain â†’ need more data
```

**Advantage:** Gives prediction-level uncertainty (clinically valuable!)

**Trade-off:** 100Ã— slower (but still fast enough for 60 ROIs)

---

## Clinical Biomarkers

### Primary Features

| Feature | Description | Expected Value |
|---------|-------------|----------------|
| `low_frac` | Fraction with D < 1.0 Î¼mÂ²/ms | High in tumor (0.5-0.8) |
| `high_frac` | Fraction with D â‰¥ 2.5 Î¼mÂ²/ms | High in normal (0.3-0.6) |
| `low_high_ratio` | **Key biomarker** | >2 suggests tumor |

### Uncertainty Features

| Feature | Description | Interpretation |
|---------|-------------|----------------|
| `low_frac_std` | Std dev across MCMC samples | High â†’ uncertain reconstruction |
| `low_high_ratio_std` | Uncertainty in ratio | High â†’ noisy data |
| `spectrum_uncertainty` | Global uncertainty metric | High â†’ need more b-values |

---

## Test Results (Synthetic Data)

### Test 1: Feature Extraction
âœ“ Tumor spectrum: `low_high_ratio = 350`
âœ“ Normal spectrum: `low_high_ratio = 0.25`
âœ“ Uncertainty: `low_frac = 0.70 Â± 0.05`

### Test 2: Classifier Training
âœ“ Accuracy: 1.000
âœ“ AUC: 1.000
âœ“ Top feature: `low_high_ratio` (as expected!)

### Test 3: Uncertainty Features
âœ“ Classifier uses uncertainty features (11.5% importance)
âœ“ Features ranked: `low_high_ratio` > `low_high_ratio_std` > `spectrum_uncertainty`

---

## Next Steps to Test on Real Data

### Step 1: Quick Test (5 ROIs, ~2 minutes)

```bash
cd /Users/PWR/Documents/Professional/Papers/Paper3/code/spectra-estimation-dMRI

uv run python scripts/run_bwh_biomarker_analysis.py \
  --sampler gibbs \
  --n_iter 2000 \
  --classifier logistic
```

**What to check:**
- Does it load BWH data correctly?
- Are features extracted without errors?
- Is AUC > 0.5 (better than random)?
- Check `results/bwh_biomarker_analysis/feature_importance.pdf`

### Step 2: Full BWH Dataset (~30 minutes)

```bash
# First, remove the [:5] limit in run_bwh_biomarker_analysis.py line 88
# Change: for i, signal_decay in enumerate(samples_with_labels[:5]):
# To:     for i, signal_decay in enumerate(samples_with_labels):

uv run python scripts/run_bwh_biomarker_analysis.py \
  --sampler gibbs \
  --n_iter 5000 \
  --classifier logistic
```

**Expected outputs:**
- `biomarker_results.csv` - Features + predictions for all ROIs
- `feature_importance.pdf` - Which features matter?
- Console: Cross-validation AUC

### Step 3: Monte Carlo Predictions (if uncertainty matters)

```bash
uv run python scripts/run_bwh_biomarker_analysis.py \
  --sampler nuts \
  --n_iter 5000 \
  --use_mc \
  --classifier logistic
```

**Expected outputs:**
- `mc_predictions.pdf` - Predictions with error bars
- Higher AUC if uncertainty helps

### Step 4: Compare Samplers

```bash
# Run both
uv run python scripts/run_bwh_biomarker_analysis.py --sampler gibbs --n_iter 5000
uv run python scripts/run_bwh_biomarker_analysis.py --sampler nuts --n_iter 5000

# Compare:
# - Which gives tighter uncertainty estimates?
# - Which has better cross-validation AUC?
# - Which features are most important in each?
```

---

## Integration with Main Pipeline

To enable in `main.py`:

```yaml
# In your config file
biomarker_analysis:
  enabled: true
  use_uncertainty: true
  use_mc_predictions: false

classifier:
  name: logistic  # or 'random_forest'

dataset:
  name: bwh
  metadata_path: src/spectra_estimation_dmri/data/bwh/metadata.csv
```

Then run:
```bash
uv run python src/spectra_estimation_dmri/main.py \
  dataset=bwh \
  inference=gibbs \
  biomarker_analysis.enabled=true
```

---

## What Makes This Implementation Good

1. **Simple** - No complex class hierarchies, just functions
2. **Modular** - Can use independently of main pipeline
3. **Principled** - Two approaches (fast + principled)
4. **Tested** - All tests pass with synthetic data
5. **Documented** - Clear guide and examples
6. **Practical** - Focuses on clinical biomarkers (low_high_ratio)
7. **Uncertainty-aware** - Uses MCMC samples properly

---

## Questions This Can Answer

- [ ] Does `low_high_ratio` discriminate between Gleason scores?
- [ ] Are uncertainty features important for classification?
- [ ] Does Monte Carlo prediction improve over point estimates?
- [ ] Which MCMC sampler (Gibbs/NUTS) gives better biomarkers?
- [ ] How many MCMC iterations are needed for stable biomarkers?
- [ ] Can we predict high-grade cancer better than ADC alone?

**Ready to find out!** ðŸŽ¯

